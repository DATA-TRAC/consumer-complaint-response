{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "# explore\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "# nlp\n",
    "import unicodedata\n",
    "import re\n",
    "import nltk\n",
    "from wordcloud import WordCloud\n",
    "from nltk.corpus import stopwords\n",
    "import nltk.sentiment\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# modeling\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import naive_bayes as nb\n",
    "from sklearn.feature_selection import SelectKBest,chi2,mutual_info_classif\n",
    "\n",
    "# local\n",
    "import wrangle as w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acquire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.oauth2 import service_account\n",
    "# import pandas_gbq\n",
    "\n",
    "# credentials = service_account.Credentials.from_service_account_file(\n",
    "#     'my-ds-projects-d864a770b51b.json',\n",
    "# )\n",
    "# df = pandas_gbq.read_gbq(\"select * from cfpb_complaints.complaint_database\",dialect='standard',project_id=\"my-ds-projects\",use_bqstorage_api=True,credentials=credentials)\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = w.check_file_exists_gbq('cfpb.csv','service_key.json')\n",
    "# df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_clean = w.clean_data(df.copy())\n",
    "# df_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_clean.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option('display.max_colwidth',None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_clean_sample = prep_narrative(df_clean.head(10000))\n",
    "# df_clean_sample[['narrative','clean','lemon']].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_words = [word for row in df_clean_sample['clean'] for word in row.split()]\n",
    "# all_words = all_words.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_words.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_str = df_clean.narrative[0]\n",
    "# sample_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_str_sub = re.sub(r'[X{1,}\\d\\']', ' ', string=sample_str)\n",
    "# sample_str_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_str_clean = basic_clean(sample_str_sub)\n",
    "# sample_str_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_str_token = token_it_up(sample_str_clean)\n",
    "# sample_str_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_str_stop = remove_stopwords(sample_str_token,[\"&#9;\", \"12\", \"'\"])\n",
    "# sample_str_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_str_lem = lemmad(sample_str_stop)\n",
    "# sample_str_lem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_lem = w.prep_narrative(df_clean)\n",
    "# df_lem.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_lem.to_parquet('df_lem.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_lem = pd.read_parquet('df_lem.parquet')\n",
    "# df_lem.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_lem = pd.read_parquet('df_prep.parquet')\n",
    "# df_lem.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_lem.company_response_to_consumer.value_counts(normalize=True).plot(\n",
    "#     kind='barh',\n",
    "#     title='Percent of Company Responses to Customer',\n",
    "#     xlabel='Percent of Responses',\n",
    "#     ylabel='Company Response');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_lem.company_response_to_consumer.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_words = [word for row in df_lem['lemon'] for word in row.split()]\n",
    "# all_words.sort()\n",
    "# all_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lem = pd.read_parquet('df_prep.parquet')\n",
    "df_lem.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, validate, test = w.split_data(df_lem,'company_response_to_consumer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.company_response_to_consumer.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate.company_response_to_consumer.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test.company_response_to_consumer.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.Which product is more likely to have monetary relief?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "money = train[train.company_response_to_consumer=='Closed with monetary relief']\n",
    "money.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "money.product_bins.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "money.product_bins.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross = pd.crosstab(train['product_bins'],train['company_response_to_consumer'],normalize='index')\n",
    "cross"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross['Closed with monetary relief'].sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross['Closed with monetary relief'].sort_values().plot(kind='barh', title='Proportions of Monetary Relief', xlabel='Proportion of Complaints for the Product', ylabel='Product Type');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# credit card and bank related products have the highest chance of getting monetary relief at just under 20%\n",
    "# it makes sense that credit report products have the least chance of getting monetary relief"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monetary_product(train):\n",
    "    # make crosstab of product and responses and normalize to get product proportions\n",
    "    cross = pd.crosstab(train['product_bins'],train['company_response_to_consumer'],normalize='index')\n",
    "    # plot monetary relief products\n",
    "    cross['Closed with monetary relief'].sort_values(\n",
    "        ).plot(kind='barh', \n",
    "                title='Proportions of Monetary Relief', \n",
    "                xlabel='Proportion of Complaints for the Product', \n",
    "                ylabel='Product Type');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monetary_product(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Are there more complaints during certain seasons of the year?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train['month'] = train.date_received.apply(lambda row: row.strftime(\"%m\")).astype(str)\n",
    "# train['year'] = train.date_received.apply(lambda row: row.strftime(\"%y\")).astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Performed 1 aggregation grouped on columns: 'month', 'year'\n",
    "# yearly = train.groupby(['month', 'year']).agg(year_count=('year', 'count'))\n",
    "\n",
    "# # Performed 1 aggregation grouped on column: 'month'\n",
    "# monthly = yearly.groupby(['month']).agg(year_count_mean=('year_count', 'mean'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.groupby(['month']).agg(month_count=('month', 'count')).plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model as m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(df):\n",
    "    '''Encode categorical columns'''\n",
    "    # columns to encode\n",
    "    cols = ['tags','product_bins']\n",
    "    # encode the dummies\n",
    "    dummy = pd.get_dummies(df[cols],prefix='',prefix_sep='',drop_first=True)\n",
    "    # bring the dummies along\n",
    "    return pd.concat([df,dummy],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = encode(train)\n",
    "X_train = X_train.drop(columns=['date_received','company_response_to_consumer','clean','state','company_name','tags','product_bins'])\n",
    "y_train = train['company_response_to_consumer']\n",
    "X_val = encode(validate)\n",
    "X_val = X_val.drop(columns=['date_received','company_response_to_consumer','clean','state','company_name','tags','product_bins'])\n",
    "y_val = validate['company_response_to_consumer']\n",
    "X_test = encode(test)\n",
    "X_test = X_test.drop(columns=['date_received','company_response_to_consumer','clean','state','company_name','tags','product_bins'])\n",
    "y_test = test['company_response_to_consumer']\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_cv(Xtr,Xv,Xt):\n",
    "    \"\"\"\n",
    "    The function `make_cv` takes in three sets of data (train, validation, and test) and converts them\n",
    "    into bag-of-words representations using a CountVectorizer with n-gram range of 1 to 3, and then\n",
    "    returns the transformed data as dataframes.\n",
    "    \n",
    "    :param Xtr: Xtr is the training data, which is a pandas DataFrame containing the lemmatized text\n",
    "    data\n",
    "    :param Xv: Xv is the validation dataset, which is used to evaluate the performance of the model\n",
    "    during training. It is a subset of the overall dataset that is not used for training the model but\n",
    "    is used to tune the hyperparameters and assess the model's generalization ability\n",
    "    :param Xt: Xt is the test data, which is a dataframe containing the text data that you want to\n",
    "    classify or analyze\n",
    "    :return: three dataframes: Xtr_cv, Xv_cv, and Xt_cv.\n",
    "    \"\"\"\n",
    "    #make my bag of words up to trigrams cv and keep single characters\n",
    "    cv = CountVectorizer(token_pattern=r'(?u)\\b\\w+\\b', max_features=2900)\n",
    "    # fit and transform train\n",
    "    Xtr_bow_cv = cv.fit_transform(Xtr.lemon)\n",
    "    # transform val and test\n",
    "    Xv_bow_cv = cv.transform(Xv.lemon)\n",
    "    Xt_bow_cv = cv.transform(Xt.lemon)\n",
    "    # make dfs\n",
    "    Xtr_cv = pd.DataFrame(Xtr_bow_cv.todense(),columns=cv.get_feature_names_out(),index=Xtr.index)\n",
    "    Xv_cv = pd.DataFrame(Xv_bow_cv.todense(),columns=cv.get_feature_names_out(),index=Xv.index)\n",
    "    Xt_cv = pd.DataFrame(Xt_bow_cv.todense(),columns=cv.get_feature_names_out(),index=Xt.index)\n",
    "    return Xtr_cv,Xv_cv,Xt_cv\n",
    "\n",
    "\n",
    "def make_tfidf(Xtr,Xv,Xt):\n",
    "    \"\"\"\n",
    "    The function `make_tfidf` takes in three sets of data (train, validation, and test) and applies the\n",
    "    TF-IDF vectorization technique to convert the text data into numerical features, using n-grams up to\n",
    "    trigrams and keeping single characters. It then returns the transformed data as pandas DataFrames.\n",
    "    \n",
    "    :param Xtr: Xtr is the training data, which is a dataframe containing the text data that you want to\n",
    "    transform into TF-IDF features. The \"lemmatized\" column in the dataframe contains the preprocessed\n",
    "    text data\n",
    "    :param Xv: Xv is the validation dataset, which is used to evaluate the performance of the model\n",
    "    during training\n",
    "    :param Xt: Xt is the input data for the test set. It is a dataframe containing the text data that\n",
    "    needs to be transformed into TF-IDF representation\n",
    "    :return: three dataframes: Xtr_tfidf, Xv_tfidf, and Xt_tfidf.\n",
    "    \"\"\"\n",
    "    #make my bag of words up to trigrams tfidf and keep single characters\n",
    "    tfidf = TfidfVectorizer(token_pattern=r'(?u)\\b\\w+\\b', max_features=2900)\n",
    "    # fit and transform train\n",
    "    Xtr_bow_tfidf = tfidf.fit_transform(Xtr.lemon)\n",
    "    # transform val and test\n",
    "    Xv_bow_tfidf = tfidf.transform(Xv.lemon)\n",
    "    Xt_bow_tfidf = tfidf.transform(Xt.lemon)\n",
    "    # make dfs\n",
    "    Xtr_tfidf = pd.DataFrame(Xtr_bow_tfidf.todense(),columns=tfidf.get_feature_names_out(),index=Xtr.index)\n",
    "    Xv_tfidf = pd.DataFrame(Xv_bow_tfidf.todense(),columns=tfidf.get_feature_names_out(),index=Xv.index)\n",
    "    Xt_tfidf = pd.DataFrame(Xt_bow_tfidf.todense(),columns=tfidf.get_feature_names_out(),index=Xt.index)\n",
    "    return Xtr_tfidf,Xv_tfidf,Xt_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cv,X_val_cv,X_test_cv = make_cv(X_train[['lemon']], X_val[['lemon']], X_test[['lemon']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tf,X_val_tf,X_test_tf = make_tfidf(X_train, X_val, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cve = pd.concat([X_train.iloc[:,1:],X_train_cv],left_index=True, right_index=True)\n",
    "X_train_cve.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_cve = pd.concat([X_val.iloc[:,1:],X_val_cv],axis=1)\n",
    "X_test_cve = pd.concat([X_test.iloc[:,1:],X_test_cv],axis=1)\n",
    "X_train_tfe = pd.concat([X_train.iloc[:,1:],X_train_tf],axis=1)\n",
    "X_val_tfe = pd.concat([X_val.iloc[:,1:],X_val_tf],axis=1)\n",
    "X_test_tfe = pd.concat([X_test.iloc[:,1:],X_test_tf],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_kbest(X, y, k=2, scoring=chi2):\n",
    "    '''\n",
    "    will take in two pandas objects:\n",
    "    X: a dataframe representing numerical independent features\n",
    "    y: a pandas Series representing a target variable\n",
    "    k: a keyword argument defaulted to 2 for the number of ideal features we elect to select\n",
    "    scoring: scoring type, default chi2, other category mutual_info_classif\n",
    "    ---\n",
    "    return: a df of the selected features from the SelectKBest process\n",
    "    ---\n",
    "    Format: kbest_results = function()\n",
    "    '''\n",
    "    kbest = SelectKBest(scoring, k=k)\n",
    "    kbest.fit(X, y)\n",
    "    mask = kbest.get_support()\n",
    "    kbest_results = pd.DataFrame(\n",
    "                dict(p_value=kbest.pvalues_, feature_score=kbest.scores_),\n",
    "                index = X.columns)\n",
    "    return kbest_results.sort_values(by=['feature_score'], ascending=False).head(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_models(Xtr,ytr,Xv,yv):\n",
    "    metrics = []\n",
    "    # cycle through depth,leaf,class_weight for dec tree\n",
    "    for d,l,cw in itertools.combinations(range(1,21),range(1,21),['balanced',None]):\n",
    "        # decision tree\n",
    "        tree = DecisionTreeClassifier(max_depth=d, min_samples_leaf=l,class_weight=cw,random_state=123)\n",
    "        tree.fit(Xtr,ytr)\n",
    "        # accuracies\n",
    "        ytr_acc = tree.score(Xtr,ytr)\n",
    "        yv_acc = tree.score(Xv,yv)\n",
    "        # table-ize\n",
    "        output ={\n",
    "                'model':'Decision Tree',\n",
    "                'params':f\"max_depth={d},min_samples_leaf={l},class_weight={cw},random_state=123\",\n",
    "                'tr_acc':ytr_acc,\n",
    "                'v_acc':yv_acc,\n",
    "            }\n",
    "        metrics.append(output)\n",
    "    return pd.DataFrame(metrics)\n",
    "\n",
    "def forest_models(Xtr,ytr,Xv,yv):\n",
    "    metrics = []\n",
    "    # cycle through depth,leaf,class_weight for random forest\n",
    "    for d,l,cw in itertools.combinations(range(1,21),range(1,21),['balanced','balanced_subsample',None]):\n",
    "        # random forest\n",
    "        forest = RandomForestClassifier(max_depth=d, min_samples_leaf=l,class_weight=cw,random_state=123)\n",
    "        forest.fit(Xtr,ytr)\n",
    "        # accuracies\n",
    "        ytr_acc = forest.score(Xtr,ytr)\n",
    "        yv_acc = forest.score(Xv,yv)\n",
    "        # table-ize\n",
    "        output ={\n",
    "                'model':'Random Forest',\n",
    "                'params':f\"max_depth={d},min_samples_leaf={l},class_weight={cw},random_state=123\",\n",
    "                'tr_acc':ytr_acc,\n",
    "                'v_acc':yv_acc,\n",
    "            }\n",
    "        metrics.append(output)\n",
    "    return pd.DataFrame(metrics)\n",
    "\n",
    "def knn_models(Xtr,ytr,Xv,yv):\n",
    "    metrics = []\n",
    "    # cycle through neighbors and weights for knn\n",
    "    for n,w in itertools.combinations(range(1,21),['uniform', 'distance']):\n",
    "        # knn\n",
    "        forest = KNeighborsClassifier(n_neighbors=n,weights=w)\n",
    "        forest.fit(Xtr,ytr)\n",
    "        # accuracies\n",
    "        ytr_acc = forest.score(Xtr,ytr)\n",
    "        yv_acc = forest.score(Xv,yv)\n",
    "        # table-ize\n",
    "        output ={\n",
    "                'model':'KNN',\n",
    "                'params':f\"n_neighbors={n},weights={w}\",\n",
    "                'tr_acc':ytr_acc,\n",
    "                'v_acc':yv_acc,\n",
    "            }\n",
    "        metrics.append(output)\n",
    "    return pd.DataFrame(metrics)\n",
    "\n",
    "def log_models(Xtr,ytr,Xv,yv):\n",
    "    metrics = []\n",
    "    # cycle through C,class_weight for log reg\n",
    "    for c,cw in itertools.combinations([.01,.1,1,10,100,1000],['balanced',None]):\n",
    "        # logistic regression\n",
    "        lr = LogisticRegression(C=c,class_weight=cw,random_state=123,max_iter=500)\n",
    "        lr.fit(Xtr,ytr)\n",
    "        # accuracies\n",
    "        ytr_acc = lr.score(Xtr,ytr)\n",
    "        yv_acc = lr.score(Xv,yv)\n",
    "        # table-ize\n",
    "        output ={\n",
    "                'model':'LogReg',\n",
    "                'params':f\"C={c},class_weight={cw},random_state=123,max_iter=500\",\n",
    "                'tr_acc':ytr_acc,\n",
    "                'v_acc':yv_acc,\n",
    "            }\n",
    "        metrics.append(output)\n",
    "    return pd.DataFrame(metrics)\n",
    "\n",
    "def comp_nb_models(Xtr,ytr,Xv,yv):\n",
    "    # naive bayes complement\n",
    "    cnb = nb.ComplementNB(alpha=0,force_alpha=True)\n",
    "    cnb.fit(Xtr,ytr)\n",
    "    # accuracies\n",
    "    ytr_acc = cnb.score(Xtr,ytr)\n",
    "    yv_acc = cnb.score(Xv,yv)\n",
    "    # table-ize\n",
    "    output ={\n",
    "            'model':'CNB',\n",
    "            'params':f'alpha={a},force_alpha=True',\n",
    "            'tr_acc':ytr_acc,\n",
    "            'v_acc':yv_acc,\n",
    "        }\n",
    "    metrics = [output]\n",
    "    # cycle through alpha for CNB\n",
    "    for a in np.arange(.1,.6,.1):\n",
    "        # naive bayes complement\n",
    "        cnb = nb.ComplementNB(alpha=a)\n",
    "        cnb.fit(Xtr,ytr)\n",
    "        # accuracies\n",
    "        ytr_acc = cnb.score(Xtr,ytr)\n",
    "        yv_acc = cnb.score(Xv,yv)\n",
    "        # table-ize\n",
    "        output ={\n",
    "                'model':'CNB',\n",
    "                'params':f'alpha={a}',\n",
    "                'tr_acc':ytr_acc,\n",
    "                'v_acc':yv_acc,\n",
    "            }\n",
    "        metrics.append(output)\n",
    "    return pd.DataFrame(metrics)\n",
    "\n",
    "def multi_nb_models(Xtr,ytr,Xv,yv):\n",
    "    # naive bayes multinomial\n",
    "    mnb = nb.MultinomialNB(alpha=0,force_alpha=True)\n",
    "    mnb.fit(Xtr,ytr)\n",
    "    # accuracies\n",
    "    ytr_acc = mnb.score(Xtr,ytr)\n",
    "    yv_acc = mnb.score(Xv,yv)\n",
    "    # table-ize\n",
    "    output ={\n",
    "            'model':'MNB',\n",
    "            'params':f'alpha={a},force_alpha=True',\n",
    "            'tr_acc':ytr_acc,\n",
    "            'v_acc':yv_acc,\n",
    "        }\n",
    "    metrics = [output]\n",
    "    # cycle through alpha for MNB\n",
    "    for a in np.arange(.1,.6,.1):\n",
    "        # naive bayes multinomial\n",
    "        mnb = nb.MultinomialNB(alpha=a)\n",
    "        mnb.fit(Xtr,ytr)\n",
    "        # accuracies\n",
    "        ytr_acc = mnb.score(Xtr,ytr)\n",
    "        yv_acc = mnb.score(Xv,yv)\n",
    "        # table-ize\n",
    "        output ={\n",
    "                'model':'MNB',\n",
    "                'params':f'alpha={a}',\n",
    "                'tr_acc':ytr_acc,\n",
    "                'v_acc':yv_acc,\n",
    "            }\n",
    "        metrics.append(output)\n",
    "    return pd.DataFrame(metrics)\n",
    "\n",
    "def cat_nb_models(Xtr,ytr,Xv,yv):\n",
    "    # naive bayes categorical\n",
    "    cat = nb.CategoricalNB(alpha=0,force_alpha=True)\n",
    "    cat.fit(Xtr,ytr)\n",
    "    # accuracies\n",
    "    ytr_acc = cat.score(Xtr,ytr)\n",
    "    yv_acc = cat.score(Xv,yv)\n",
    "    # table-ize\n",
    "    output ={\n",
    "            'model':'CatNB',\n",
    "            'params':f'alpha={a},force_alpha=True',\n",
    "            'tr_acc':ytr_acc,\n",
    "            'v_acc':yv_acc,\n",
    "        }\n",
    "    metrics = [output]\n",
    "    # cycle through alpha for CatNB\n",
    "    for a in np.arange(.1,.6,.1):\n",
    "        # naive bayes categorical\n",
    "        cat = nb.CategoricalNB(alpha=a)\n",
    "        cat.fit(Xtr,ytr)\n",
    "        # accuracies\n",
    "        ytr_acc = cat.score(Xtr,ytr)\n",
    "        yv_acc = cat.score(Xv,yv)\n",
    "        # table-ize\n",
    "        output ={\n",
    "                'model':'CatNB',\n",
    "                'params':f'alpha={a}',\n",
    "                'tr_acc':ytr_acc,\n",
    "                'v_acc':yv_acc,\n",
    "            }\n",
    "        metrics.append(output)\n",
    "    return pd.DataFrame(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
